{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jadi Tambah Jumlah Tweet deng Form pickle fix\n",
    "import tweepy\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from numpy import array\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import nltk\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "#%matplotlib inline\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "import plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pickle\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "api_key = \"\"\n",
    "api_secret_key = \"\"\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\"\n",
    "\n",
    "authentication = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "authentication.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(authentication, wait_on_rate_limit=True)\n",
    "\n",
    "def get_related_tweets(text_query):\n",
    "    tweets_list = []\n",
    "    count = 250\n",
    "    try:\n",
    "        #parameter search keyword\n",
    "        for tweet in tweepy.Cursor(api.search, q=text_query, lang='id',tweet_mode = 'extended', trucated='false').items(100):\n",
    "        #cleansing retweet\n",
    "                tweettext = str(tweet.full_text.encode('ascii',errors='ignore'))\n",
    "                if tweettext.startswith(\"rt @\") == False and hasattr(tweet, 'retweeted_status') == False:\n",
    "                    tweets_list.append({'created_at': tweet.created_at,\n",
    "                                               'tweet_id': tweet.id,\n",
    "                                               'tweet_text': tweet.full_text})\n",
    "                    \n",
    "                    tweetx = pd.DataFrame.from_dict(tweets_list)\n",
    "                    tweetx2 = pd.DataFrame.from_dict(tweets_list)\n",
    "                    tweetxy = tweetx['tweet_text']\n",
    "                    tweettanggall = tweetx['created_at'].astype(str)\n",
    "                    tweettanggal = str(tweetx['created_at'])\n",
    "                    \n",
    "                    #cleansing\n",
    "                    tweetx2['tweet_text'] =  tweetx2['tweet_text'].str.replace(r'[@#][A-Za-z0-9_]+', '')\n",
    "                    tweetx2['tweet_text'] =  tweetx2['tweet_text'].str.replace('https://', '')\n",
    "                    tweetx2['tweet_text'] =  tweetx2['tweet_text'].str.replace('t.co/[a-zA-z0-9]+', '')\n",
    "                    tweetx2['tweet_text'] =  tweetx2['tweet_text'].str.lstrip(':b')\n",
    "                    tweetx2['tweet_text'] =  tweetx2['tweet_text'].str.replace(r'\\\\x[a-z0-9]+', '')\n",
    "                    tweetx2['tweet_text'] =  tweetx2['tweet_text'].str.replace(r'\\\\n', ' ')\n",
    "                    tweetx2['tweet_text'] =  tweetx2['tweet_text'].str.replace(r'[^\\w\\s]',' ', re.UNICODE)\n",
    "                    tweetx2['tweet_text'] =  tweetx2['tweet_text'].str.replace(r'[-._]',' ', re.UNICODE)\n",
    "                    tweetx2['tweet_text'] =  tweetx2['tweet_text'].str.replace(r'[0-9]+', '')\n",
    "                    #case folding\n",
    "                    tweetx2['tweet_text'] =  tweetx2['tweet_text'].str.lower()\n",
    "                    #token\n",
    "                    tweetx2['tweet_text'] =  tweetx2['tweet_text'].apply(word_tokenize)\n",
    "                    #stopword\n",
    "                    factory = StopWordRemoverFactory()\n",
    "                    stopwords = factory.get_stop_words()\n",
    "                    stopwords.extend(['g','ae', 'y', 'a', 'i','ye', 'ya', 'da', 'yg','hm','hmm' , 'rt', \n",
    "                                      'amp', 'bilang', 'krn', 'ny','nya', 'd', 'tuh', 'utk', 'ya', 'n', \n",
    "                                      'hehe', 'loh', 'rt', '&amp','amp','yah'])\n",
    "                    stopwords = set(stopwords) - {'tidak','ada', 'belum', 'mengapa', 'kenapa', 'boleh' , \n",
    "                                                  'nggak', 'tolong', 'ingin', \n",
    "                                                  'bisa', 'nanti','dahulu', 'bilang', 'kecuali', 'tentang',\n",
    "                                                  'tetapi', 'dapat',  'daripada'}\n",
    "                    tweetx2['tweet_text'] =  tweetx2['tweet_text'].apply(lambda x: [item for item in x if \n",
    "                                                                                    item not in stopwords])\n",
    "                    tweetx2.to_csv('data2.csv')\n",
    "                    datacsv = pd.read_csv('data2.csv')\n",
    "                    \n",
    "        return tweetx, tweettanggal, tweetxy ,tweettanggall, datacsv, text_query;\n",
    "    except BaseException as e:\n",
    "        print('failed on_status,', str(e))\n",
    "        time.sleep(3)\n",
    "\n",
    "        \n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def home():\n",
    "    if request.method == \"GET\":\n",
    "        return render_template(\"htmlforscrapping.html\")\n",
    "    elif request.method == \"POST\":\n",
    "        user = request.form['name']\n",
    "        return redirect(url_for('success', name=user))\n",
    "    \n",
    "@app.route('/success/<name>', methods=[\"GET\", \"POST\"])\n",
    "def success(name):\n",
    "    if request.method == \"GET\":\n",
    "        global model,graph\n",
    "        graph = tf.get_default_graph()\n",
    "        model = load_model('weights3.hdf5')\n",
    "      \n",
    "        #tokenizer, index, padding\n",
    "        tweetx, tweetxy, tweettanggal, tweettanggall, datacsv, text_query = get_related_tweets(name)\n",
    "        que = text_query\n",
    "        \n",
    "        with open('tokenizer1.pcl', 'rb') as handle:\n",
    "            tokenizer2 = pickle.load(handle)\n",
    "        #print(que)\n",
    "        x_test = tokenizer2.texts_to_sequences(datacsv['tweet_text'])\n",
    "        x_test = pad_sequences(x_test, maxlen=47)\n",
    "        \n",
    "        hasil = model.predict(x_test)\n",
    "        prediksi2 = np.argmax(hasil, axis=1)\n",
    "        hasilprediksi = []\n",
    "        for i in range(len(prediksi2)):\n",
    "            if prediksi2[i] == 2:\n",
    "                hasilprediksi.append(\"Positif\")\n",
    "            elif prediksi2[i] == 1:\n",
    "                hasilprediksi.append(\"Netral\")\n",
    "            else:\n",
    "                hasilprediksi.append(\"Negatif\")  \n",
    "        \n",
    "        #pie chart\n",
    "        Positif, Netral, Negatif = 0, 0, 0\n",
    "        for i, prediksi in enumerate(hasilprediksi):\n",
    "            if prediksi==\"Positif\":\n",
    "                Positif += 1\n",
    "            elif prediksi==\"Netral\":\n",
    "                Netral += 1\n",
    "            else:\n",
    "                Negatif += 1   \n",
    "        \n",
    "        labels=['[Positif]','[Netral]', '[Negatif]']\n",
    "        sizes=[Positif, Netral, Negatif]   \n",
    "        graph_values = [{\n",
    "                    'labels': labels,\n",
    "                    'values': sizes,\n",
    "                    'type': 'pie',\n",
    "                    'insidetextfont': {'color': '#FFFFFF',\n",
    "                                        'size': '14',\n",
    "                    },\n",
    "                    'textfont': {'color': '#FFFFFF',\n",
    "                                        'size': '14',\n",
    "                    },\n",
    "                    }]\n",
    "\n",
    "        layout = {\n",
    "                'title': '<b>Rekapitulasi Sentimen</b>',\n",
    "\n",
    "                }\n",
    "        \n",
    "        #for keyword in each tweet(database csv)\n",
    "        hasiltweet = tweetx['tweet_text']\n",
    "        hasiltweets = hasiltweet.tolist()\n",
    "        q_text = []\n",
    "        for z in range (len(hasiltweets)):\n",
    "            q_text.append(que)\n",
    "        \n",
    "        headings = (\"Tanggal\", \"Tweet\", \"Sentimen\")\n",
    "        data = list(zip(tweettanggall, hasiltweets, hasilprediksi))\n",
    "    \n",
    "        dict1 = {\"Keyword\": q_text, \"Tanggal\": tweettanggall, \"Tweet\": hasiltweets, \"Sentimen\": \n",
    "                 hasilprediksi}\n",
    "        df = pd.DataFrame(dict1, columns= [\"Keyword\", \"Tanggal\", \"Tweet\", \"Sentimen\"])\n",
    "        df.to_csv('tes3.csv', mode='a', header=False)\n",
    "    \n",
    "        #Bekeng line chart\n",
    "        df = pd.read_csv('tes3.csv', header=None, \n",
    "                 names=[\"Keyword\", \"Tanggal\", \"Tweet\", \"Sentimen\"])\n",
    "        df['Tanggal'] = pd.to_datetime(df['Tanggal'], errors='coerce')\n",
    "\n",
    "\n",
    "        mask = df['Keyword']==que\n",
    "        result = df[mask].groupby([pd.Grouper(key='Tanggal', freq='60s'), 'Sentimen']).count().unstack(fill_value=0).stack().reset_index()\n",
    "        result = result.rename(columns={\"Keyword\": \"Jumlah Sentimen\", \"Tanggal\":\"Waktu\"})\n",
    "        time_series = result[\"Waktu\"][result['Sentimen']==0].reset_index(drop=True)\n",
    "\n",
    "\n",
    "        fig = px.line(result, x='Waktu',                      \\\n",
    "              y=\"Jumlah Sentimen\",\n",
    "              color=\"Sentimen\")      \n",
    "    \n",
    "        fig = fig.update_xaxes(rangeslider_visible=True)\n",
    "        fig.update_layout(width=750, height=500)\n",
    "        graphJSON = json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder)\n",
    "    \n",
    "        return render_template(\"fortabel.html\", headings=headings, data=data, \n",
    "                               que=que, graph_values=graph_values, layout=layout, \n",
    "                               graphJSON=graphJSON)\n",
    "    \n",
    "    elif request.method == \"POST\":\n",
    "        user = request.form['nama']\n",
    "        return redirect(url_for('success', name=user))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
